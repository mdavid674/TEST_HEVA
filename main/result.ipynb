{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical test results for HEVA company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook repeats the statement of the test. Under each activity you will find the code and the result produced.\n",
    "You will find all the requirements to run this notebook in the requirements.md file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of necessary parameters\n",
    "data_path = \"../sources/data/movies.sqlite\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    \"\"\" Configuring the Pyspark session with the jdbc package\n",
    "        to read the \"movies.sqlite\" file.\n",
    "\n",
    "    Args:\n",
    "        data_path (string): The sqlite data file path\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of 2 Pyspark Dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    # Creation of the Spark session\n",
    "    spark = SparkSession.builder\\\n",
    "        .config(\n",
    "            'spark.jars.packages',\n",
    "            'org.xerial:sqlite-jdbc:3.34.0')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Reading the movies table\n",
    "    df_movies = spark.read.format('jdbc')\\\n",
    "        .options(\n",
    "            driver='org.sqlite.JDBC',\n",
    "            dbtable='movies',\n",
    "            url=f'jdbc:sqlite:{data_path}')\\\n",
    "        .load()\n",
    "\n",
    "    # Reading the ratings table\n",
    "    df_ratings = spark.read.format('jdbc')\\\n",
    "        .options(\n",
    "            driver='org.sqlite.JDBC',\n",
    "            dbtable='ratings',\n",
    "            url=f'jdbc:sqlite:{data_path}')\\\n",
    "        .load()\n",
    "\n",
    "    return df_movies, df_ratings\n",
    "\n",
    "\n",
    "df_movies, df_ratings = read_data(data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('test_heva')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64a6df10e2c5214e0607b70b4fb1785e65c456952a5c675fa786aef034d7f982"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
